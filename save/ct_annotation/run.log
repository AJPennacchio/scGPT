14:08:30-scGPT-INFO-<module>: match 17578/17818 genes in vocabulary of size 60697.
14:08:31-scGPT-INFO-<module>: Resume model from models/scGPT_brain/best_model.pt, the model args will override the config models/scGPT_brain/args.json.
14:11:24-scGPT-INFO-__call__: Filtering cells by counts ...
14:12:04-scGPT-INFO-__call__: Normalizing total counts ...
14:12:09-scGPT-INFO-__call__: Binning data ...
14:18:56-scGPT-INFO-__call__: Filtering cells by counts ...
14:19:11-scGPT-INFO-__call__: Normalizing total counts ...
14:19:12-scGPT-INFO-__call__: Binning data ...
14:23:57-scGPT-INFO-<module>: train set number of samples: 495804, 
	 feature length: 3001
14:23:57-scGPT-INFO-<module>: valid set number of samples: 55090, 
	 feature length: 3001
14:24:05-scGPT-INFO-<module>: Loading params encoder.embedding.weight with shape torch.Size([60697, 512])
14:24:05-scGPT-INFO-<module>: Loading params encoder.enc_norm.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params encoder.enc_norm.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])
14:24:05-scGPT-INFO-<module>: Loading params value_encoder.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params value_encoder.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params value_encoder.norm.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params value_encoder.norm.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params decoder.fc.0.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params decoder.fc.0.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params decoder.fc.2.weight with shape torch.Size([512, 512])
14:24:05-scGPT-INFO-<module>: Loading params decoder.fc.2.bias with shape torch.Size([512])
14:24:05-scGPT-INFO-<module>: Loading params decoder.fc.4.weight with shape torch.Size([1, 512])
14:24:05-scGPT-INFO-<module>: Loading params decoder.fc.4.bias with shape torch.Size([1])
14:24:05-scGPT-INFO-<module>: Total Pre freeze Params 51339280
14:24:05-scGPT-INFO-<module>: Total Post freeze Params 51339280
14:25:54-scGPT-INFO-train: | epoch   1 | 100/15494 batches | lr 0.0001 | ms/batch 729.04 | loss  0.57 | cls  0.57 | err  0.15 | 
14:26:44-scGPT-INFO-train: | epoch   1 | 200/15494 batches | lr 0.0001 | ms/batch 496.77 | loss  0.22 | cls  0.22 | err  0.06 | 
14:27:34-scGPT-INFO-train: | epoch   1 | 300/15494 batches | lr 0.0001 | ms/batch 506.25 | loss  0.22 | cls  0.22 | err  0.06 | 
14:28:24-scGPT-INFO-train: | epoch   1 | 400/15494 batches | lr 0.0001 | ms/batch 500.85 | loss  0.20 | cls  0.20 | err  0.05 | 
14:29:14-scGPT-INFO-train: | epoch   1 | 500/15494 batches | lr 0.0001 | ms/batch 496.27 | loss  0.18 | cls  0.18 | err  0.05 | 
14:30:04-scGPT-INFO-train: | epoch   1 | 600/15494 batches | lr 0.0001 | ms/batch 497.16 | loss  0.20 | cls  0.20 | err  0.05 | 
14:30:54-scGPT-INFO-train: | epoch   1 | 700/15494 batches | lr 0.0001 | ms/batch 504.44 | loss  0.18 | cls  0.18 | err  0.05 | 
14:31:45-scGPT-INFO-train: | epoch   1 | 800/15494 batches | lr 0.0001 | ms/batch 504.53 | loss  0.16 | cls  0.16 | err  0.05 | 
14:32:35-scGPT-INFO-train: | epoch   1 | 900/15494 batches | lr 0.0001 | ms/batch 502.77 | loss  0.18 | cls  0.18 | err  0.05 | 
14:33:25-scGPT-INFO-train: | epoch   1 | 1000/15494 batches | lr 0.0001 | ms/batch 504.25 | loss  0.17 | cls  0.17 | err  0.04 | 
14:34:15-scGPT-INFO-train: | epoch   1 | 1100/15494 batches | lr 0.0001 | ms/batch 500.55 | loss  0.16 | cls  0.16 | err  0.04 | 
14:35:05-scGPT-INFO-train: | epoch   1 | 1200/15494 batches | lr 0.0001 | ms/batch 497.72 | loss  0.17 | cls  0.17 | err  0.05 | 
14:35:55-scGPT-INFO-train: | epoch   1 | 1300/15494 batches | lr 0.0001 | ms/batch 496.82 | loss  0.20 | cls  0.20 | err  0.05 | 
14:36:45-scGPT-INFO-train: | epoch   1 | 1400/15494 batches | lr 0.0001 | ms/batch 499.49 | loss  0.16 | cls  0.16 | err  0.04 | 
14:37:35-scGPT-INFO-train: | epoch   1 | 1500/15494 batches | lr 0.0001 | ms/batch 499.62 | loss  0.17 | cls  0.17 | err  0.04 | 
14:38:25-scGPT-INFO-train: | epoch   1 | 1600/15494 batches | lr 0.0001 | ms/batch 499.10 | loss  0.16 | cls  0.16 | err  0.04 | 
14:39:14-scGPT-INFO-train: | epoch   1 | 1700/15494 batches | lr 0.0001 | ms/batch 496.63 | loss  0.15 | cls  0.15 | err  0.04 | 
14:40:05-scGPT-INFO-train: | epoch   1 | 1800/15494 batches | lr 0.0001 | ms/batch 505.18 | loss  0.16 | cls  0.16 | err  0.04 | 
14:40:55-scGPT-INFO-train: | epoch   1 | 1900/15494 batches | lr 0.0001 | ms/batch 502.29 | loss  0.15 | cls  0.15 | err  0.04 | 
14:41:45-scGPT-INFO-train: | epoch   1 | 2000/15494 batches | lr 0.0001 | ms/batch 500.22 | loss  0.16 | cls  0.16 | err  0.04 | 
14:42:36-scGPT-INFO-train: | epoch   1 | 2100/15494 batches | lr 0.0001 | ms/batch 505.33 | loss  0.15 | cls  0.15 | err  0.04 | 
14:43:25-scGPT-INFO-train: | epoch   1 | 2200/15494 batches | lr 0.0001 | ms/batch 498.11 | loss  0.14 | cls  0.14 | err  0.04 | 
14:44:15-scGPT-INFO-train: | epoch   1 | 2300/15494 batches | lr 0.0001 | ms/batch 500.58 | loss  0.14 | cls  0.14 | err  0.04 | 
14:45:06-scGPT-INFO-train: | epoch   1 | 2400/15494 batches | lr 0.0001 | ms/batch 501.29 | loss  0.15 | cls  0.15 | err  0.04 | 
14:45:55-scGPT-INFO-train: | epoch   1 | 2500/15494 batches | lr 0.0001 | ms/batch 497.06 | loss  0.19 | cls  0.19 | err  0.05 | 
14:46:46-scGPT-INFO-train: | epoch   1 | 2600/15494 batches | lr 0.0001 | ms/batch 503.46 | loss  0.14 | cls  0.14 | err  0.04 | 
14:47:35-scGPT-INFO-train: | epoch   1 | 2700/15494 batches | lr 0.0001 | ms/batch 496.55 | loss  0.16 | cls  0.16 | err  0.04 | 
14:48:25-scGPT-INFO-train: | epoch   1 | 2800/15494 batches | lr 0.0001 | ms/batch 498.09 | loss  0.16 | cls  0.16 | err  0.04 | 
14:49:15-scGPT-INFO-train: | epoch   1 | 2900/15494 batches | lr 0.0001 | ms/batch 501.49 | loss  0.14 | cls  0.14 | err  0.04 | 
14:50:06-scGPT-INFO-train: | epoch   1 | 3000/15494 batches | lr 0.0001 | ms/batch 507.38 | loss  0.15 | cls  0.15 | err  0.04 | 
14:50:56-scGPT-INFO-train: | epoch   1 | 3100/15494 batches | lr 0.0001 | ms/batch 500.21 | loss  0.14 | cls  0.14 | err  0.04 | 
14:51:46-scGPT-INFO-train: | epoch   1 | 3200/15494 batches | lr 0.0001 | ms/batch 500.27 | loss  0.15 | cls  0.15 | err  0.05 | 
14:52:36-scGPT-INFO-train: | epoch   1 | 3300/15494 batches | lr 0.0001 | ms/batch 502.95 | loss  0.16 | cls  0.16 | err  0.04 | 
14:53:26-scGPT-INFO-train: | epoch   1 | 3400/15494 batches | lr 0.0001 | ms/batch 496.49 | loss  0.15 | cls  0.15 | err  0.04 | 
14:54:16-scGPT-INFO-train: | epoch   1 | 3500/15494 batches | lr 0.0001 | ms/batch 495.45 | loss  0.13 | cls  0.13 | err  0.04 | 
14:55:06-scGPT-INFO-train: | epoch   1 | 3600/15494 batches | lr 0.0001 | ms/batch 502.15 | loss  0.13 | cls  0.13 | err  0.04 | 
14:55:56-scGPT-INFO-train: | epoch   1 | 3700/15494 batches | lr 0.0001 | ms/batch 497.25 | loss  0.14 | cls  0.14 | err  0.04 | 
14:56:46-scGPT-INFO-train: | epoch   1 | 3800/15494 batches | lr 0.0001 | ms/batch 501.23 | loss  0.12 | cls  0.12 | err  0.04 | 
14:57:35-scGPT-INFO-train: | epoch   1 | 3900/15494 batches | lr 0.0001 | ms/batch 496.20 | loss  0.15 | cls  0.15 | err  0.04 | 
14:58:25-scGPT-INFO-train: | epoch   1 | 4000/15494 batches | lr 0.0001 | ms/batch 498.60 | loss  0.13 | cls  0.13 | err  0.04 | 
14:59:16-scGPT-INFO-train: | epoch   1 | 4100/15494 batches | lr 0.0001 | ms/batch 503.78 | loss  0.15 | cls  0.15 | err  0.04 | 
15:00:05-scGPT-INFO-train: | epoch   1 | 4200/15494 batches | lr 0.0001 | ms/batch 496.29 | loss  0.16 | cls  0.16 | err  0.05 | 
15:00:55-scGPT-INFO-train: | epoch   1 | 4300/15494 batches | lr 0.0001 | ms/batch 499.96 | loss  0.13 | cls  0.13 | err  0.04 | 
15:01:45-scGPT-INFO-train: | epoch   1 | 4400/15494 batches | lr 0.0001 | ms/batch 495.96 | loss  0.16 | cls  0.16 | err  0.04 | 
15:02:35-scGPT-INFO-train: | epoch   1 | 4500/15494 batches | lr 0.0001 | ms/batch 497.92 | loss  0.12 | cls  0.12 | err  0.04 | 
15:03:25-scGPT-INFO-train: | epoch   1 | 4600/15494 batches | lr 0.0001 | ms/batch 500.20 | loss  0.13 | cls  0.13 | err  0.04 | 
15:04:15-scGPT-INFO-train: | epoch   1 | 4700/15494 batches | lr 0.0001 | ms/batch 501.88 | loss  0.15 | cls  0.15 | err  0.05 | 
15:05:05-scGPT-INFO-train: | epoch   1 | 4800/15494 batches | lr 0.0001 | ms/batch 500.88 | loss  0.13 | cls  0.13 | err  0.04 | 
15:05:55-scGPT-INFO-train: | epoch   1 | 4900/15494 batches | lr 0.0001 | ms/batch 498.44 | loss  0.14 | cls  0.14 | err  0.04 | 
15:06:45-scGPT-INFO-train: | epoch   1 | 5000/15494 batches | lr 0.0001 | ms/batch 500.19 | loss  0.13 | cls  0.13 | err  0.04 | 
15:07:35-scGPT-INFO-train: | epoch   1 | 5100/15494 batches | lr 0.0001 | ms/batch 501.51 | loss  0.15 | cls  0.15 | err  0.04 | 
15:08:25-scGPT-INFO-train: | epoch   1 | 5200/15494 batches | lr 0.0001 | ms/batch 498.40 | loss  0.12 | cls  0.12 | err  0.04 | 
15:09:15-scGPT-INFO-train: | epoch   1 | 5300/15494 batches | lr 0.0001 | ms/batch 502.97 | loss  0.15 | cls  0.15 | err  0.04 | 
15:10:05-scGPT-INFO-train: | epoch   1 | 5400/15494 batches | lr 0.0001 | ms/batch 498.04 | loss  0.13 | cls  0.13 | err  0.04 | 
15:10:55-scGPT-INFO-train: | epoch   1 | 5500/15494 batches | lr 0.0001 | ms/batch 500.87 | loss  0.13 | cls  0.13 | err  0.04 | 
15:11:45-scGPT-INFO-train: | epoch   1 | 5600/15494 batches | lr 0.0001 | ms/batch 497.65 | loss  0.13 | cls  0.13 | err  0.03 | 
15:12:34-scGPT-INFO-train: | epoch   1 | 5700/15494 batches | lr 0.0001 | ms/batch 496.49 | loss  0.15 | cls  0.15 | err  0.04 | 
15:13:24-scGPT-INFO-train: | epoch   1 | 5800/15494 batches | lr 0.0001 | ms/batch 498.26 | loss  0.11 | cls  0.11 | err  0.03 | 
15:14:14-scGPT-INFO-train: | epoch   1 | 5900/15494 batches | lr 0.0001 | ms/batch 502.08 | loss  0.14 | cls  0.14 | err  0.04 | 
15:15:05-scGPT-INFO-train: | epoch   1 | 6000/15494 batches | lr 0.0001 | ms/batch 505.77 | loss  0.13 | cls  0.13 | err  0.03 | 
15:15:55-scGPT-INFO-train: | epoch   1 | 6100/15494 batches | lr 0.0001 | ms/batch 500.25 | loss  0.13 | cls  0.13 | err  0.04 | 
15:16:45-scGPT-INFO-train: | epoch   1 | 6200/15494 batches | lr 0.0001 | ms/batch 505.04 | loss  0.13 | cls  0.13 | err  0.04 | 
15:17:36-scGPT-INFO-train: | epoch   1 | 6300/15494 batches | lr 0.0001 | ms/batch 501.30 | loss  0.14 | cls  0.14 | err  0.04 | 
15:18:25-scGPT-INFO-train: | epoch   1 | 6400/15494 batches | lr 0.0001 | ms/batch 496.81 | loss  0.13 | cls  0.13 | err  0.04 | 
15:19:15-scGPT-INFO-train: | epoch   1 | 6500/15494 batches | lr 0.0001 | ms/batch 496.69 | loss  0.13 | cls  0.13 | err  0.04 | 
15:20:05-scGPT-INFO-train: | epoch   1 | 6600/15494 batches | lr 0.0001 | ms/batch 503.23 | loss  0.13 | cls  0.13 | err  0.04 | 
15:20:55-scGPT-INFO-train: | epoch   1 | 6700/15494 batches | lr 0.0001 | ms/batch 497.45 | loss  0.11 | cls  0.11 | err  0.04 | 
15:21:45-scGPT-INFO-train: | epoch   1 | 6800/15494 batches | lr 0.0001 | ms/batch 500.79 | loss  0.12 | cls  0.12 | err  0.03 | 
15:22:36-scGPT-INFO-train: | epoch   1 | 6900/15494 batches | lr 0.0001 | ms/batch 507.84 | loss  0.12 | cls  0.12 | err  0.04 | 
15:23:26-scGPT-INFO-train: | epoch   1 | 7000/15494 batches | lr 0.0001 | ms/batch 502.67 | loss  0.11 | cls  0.11 | err  0.04 | 
15:24:16-scGPT-INFO-train: | epoch   1 | 7100/15494 batches | lr 0.0001 | ms/batch 497.84 | loss  0.13 | cls  0.13 | err  0.04 | 
15:25:06-scGPT-INFO-train: | epoch   1 | 7200/15494 batches | lr 0.0001 | ms/batch 498.57 | loss  0.12 | cls  0.12 | err  0.03 | 
15:25:56-scGPT-INFO-train: | epoch   1 | 7300/15494 batches | lr 0.0001 | ms/batch 503.06 | loss  0.12 | cls  0.12 | err  0.04 | 
15:26:46-scGPT-INFO-train: | epoch   1 | 7400/15494 batches | lr 0.0001 | ms/batch 498.89 | loss  0.13 | cls  0.13 | err  0.04 | 
15:27:36-scGPT-INFO-train: | epoch   1 | 7500/15494 batches | lr 0.0001 | ms/batch 498.06 | loss  0.13 | cls  0.13 | err  0.04 | 
15:28:26-scGPT-INFO-train: | epoch   1 | 7600/15494 batches | lr 0.0001 | ms/batch 499.82 | loss  0.10 | cls  0.10 | err  0.03 | 
15:29:16-scGPT-INFO-train: | epoch   1 | 7700/15494 batches | lr 0.0001 | ms/batch 503.06 | loss  0.13 | cls  0.13 | err  0.04 | 
15:30:06-scGPT-INFO-train: | epoch   1 | 7800/15494 batches | lr 0.0001 | ms/batch 498.18 | loss  0.12 | cls  0.12 | err  0.04 | 
15:30:56-scGPT-INFO-train: | epoch   1 | 7900/15494 batches | lr 0.0001 | ms/batch 497.55 | loss  0.12 | cls  0.12 | err  0.03 | 
15:31:46-scGPT-INFO-train: | epoch   1 | 8000/15494 batches | lr 0.0001 | ms/batch 498.18 | loss  0.14 | cls  0.14 | err  0.04 | 
15:32:36-scGPT-INFO-train: | epoch   1 | 8100/15494 batches | lr 0.0001 | ms/batch 500.93 | loss  0.16 | cls  0.16 | err  0.04 | 
15:33:25-scGPT-INFO-train: | epoch   1 | 8200/15494 batches | lr 0.0001 | ms/batch 497.88 | loss  0.12 | cls  0.12 | err  0.04 | 
15:34:16-scGPT-INFO-train: | epoch   1 | 8300/15494 batches | lr 0.0001 | ms/batch 503.01 | loss  0.12 | cls  0.12 | err  0.03 | 
15:35:06-scGPT-INFO-train: | epoch   1 | 8400/15494 batches | lr 0.0001 | ms/batch 498.71 | loss  0.14 | cls  0.14 | err  0.04 | 
15:35:56-scGPT-INFO-train: | epoch   1 | 8500/15494 batches | lr 0.0001 | ms/batch 503.30 | loss  0.11 | cls  0.11 | err  0.03 | 
15:36:46-scGPT-INFO-train: | epoch   1 | 8600/15494 batches | lr 0.0001 | ms/batch 497.79 | loss  0.12 | cls  0.12 | err  0.04 | 
15:37:36-scGPT-INFO-train: | epoch   1 | 8700/15494 batches | lr 0.0001 | ms/batch 499.13 | loss  0.11 | cls  0.11 | err  0.04 | 
15:38:26-scGPT-INFO-train: | epoch   1 | 8800/15494 batches | lr 0.0001 | ms/batch 503.76 | loss  0.14 | cls  0.14 | err  0.04 | 
15:39:16-scGPT-INFO-train: | epoch   1 | 8900/15494 batches | lr 0.0001 | ms/batch 498.51 | loss  0.12 | cls  0.12 | err  0.04 | 
15:40:06-scGPT-INFO-train: | epoch   1 | 9000/15494 batches | lr 0.0001 | ms/batch 500.84 | loss  0.12 | cls  0.12 | err  0.03 | 
15:40:57-scGPT-INFO-train: | epoch   1 | 9100/15494 batches | lr 0.0001 | ms/batch 508.64 | loss  0.13 | cls  0.13 | err  0.03 | 
15:41:47-scGPT-INFO-train: | epoch   1 | 9200/15494 batches | lr 0.0001 | ms/batch 496.78 | loss  0.12 | cls  0.12 | err  0.04 | 
15:42:37-scGPT-INFO-train: | epoch   1 | 9300/15494 batches | lr 0.0001 | ms/batch 501.81 | loss  0.12 | cls  0.12 | err  0.04 | 
15:43:26-scGPT-INFO-train: | epoch   1 | 9400/15494 batches | lr 0.0001 | ms/batch 495.70 | loss  0.12 | cls  0.12 | err  0.04 | 
15:44:16-scGPT-INFO-train: | epoch   1 | 9500/15494 batches | lr 0.0001 | ms/batch 497.44 | loss  0.13 | cls  0.13 | err  0.04 | 
15:45:06-scGPT-INFO-train: | epoch   1 | 9600/15494 batches | lr 0.0001 | ms/batch 494.90 | loss  0.11 | cls  0.11 | err  0.03 | 
15:45:55-scGPT-INFO-train: | epoch   1 | 9700/15494 batches | lr 0.0001 | ms/batch 492.19 | loss  0.12 | cls  0.12 | err  0.03 | 
15:46:45-scGPT-INFO-train: | epoch   1 | 9800/15494 batches | lr 0.0001 | ms/batch 498.86 | loss  0.11 | cls  0.11 | err  0.03 | 
15:47:35-scGPT-INFO-train: | epoch   1 | 9900/15494 batches | lr 0.0001 | ms/batch 502.98 | loss  0.13 | cls  0.13 | err  0.04 | 
15:48:24-scGPT-INFO-train: | epoch   1 | 10000/15494 batches | lr 0.0001 | ms/batch 493.07 | loss  0.10 | cls  0.10 | err  0.03 | 
15:49:14-scGPT-INFO-train: | epoch   1 | 10100/15494 batches | lr 0.0001 | ms/batch 496.89 | loss  0.10 | cls  0.10 | err  0.03 | 
15:50:04-scGPT-INFO-train: | epoch   1 | 10200/15494 batches | lr 0.0001 | ms/batch 504.24 | loss  0.12 | cls  0.12 | err  0.04 | 
15:50:55-scGPT-INFO-train: | epoch   1 | 10300/15494 batches | lr 0.0001 | ms/batch 500.65 | loss  0.13 | cls  0.13 | err  0.04 | 
15:51:44-scGPT-INFO-train: | epoch   1 | 10400/15494 batches | lr 0.0001 | ms/batch 499.02 | loss  0.11 | cls  0.11 | err  0.04 | 
15:52:34-scGPT-INFO-train: | epoch   1 | 10500/15494 batches | lr 0.0001 | ms/batch 495.38 | loss  0.11 | cls  0.11 | err  0.03 | 
15:53:23-scGPT-INFO-train: | epoch   1 | 10600/15494 batches | lr 0.0001 | ms/batch 494.46 | loss  0.11 | cls  0.11 | err  0.03 | 
15:54:13-scGPT-INFO-train: | epoch   1 | 10700/15494 batches | lr 0.0001 | ms/batch 497.57 | loss  0.13 | cls  0.13 | err  0.04 | 
15:55:03-scGPT-INFO-train: | epoch   1 | 10800/15494 batches | lr 0.0001 | ms/batch 495.67 | loss  0.12 | cls  0.12 | err  0.04 | 
15:55:53-scGPT-INFO-train: | epoch   1 | 10900/15494 batches | lr 0.0001 | ms/batch 503.51 | loss  0.12 | cls  0.12 | err  0.03 | 
15:56:43-scGPT-INFO-train: | epoch   1 | 11000/15494 batches | lr 0.0001 | ms/batch 501.79 | loss  0.12 | cls  0.12 | err  0.04 | 
15:57:33-scGPT-INFO-train: | epoch   1 | 11100/15494 batches | lr 0.0001 | ms/batch 499.42 | loss  0.11 | cls  0.11 | err  0.03 | 
15:58:23-scGPT-INFO-train: | epoch   1 | 11200/15494 batches | lr 0.0001 | ms/batch 501.06 | loss  0.13 | cls  0.13 | err  0.04 | 
15:59:13-scGPT-INFO-train: | epoch   1 | 11300/15494 batches | lr 0.0001 | ms/batch 499.13 | loss  0.13 | cls  0.13 | err  0.04 | 
16:00:03-scGPT-INFO-train: | epoch   1 | 11400/15494 batches | lr 0.0001 | ms/batch 501.72 | loss  0.13 | cls  0.13 | err  0.04 | 
16:00:54-scGPT-INFO-train: | epoch   1 | 11500/15494 batches | lr 0.0001 | ms/batch 502.75 | loss  0.11 | cls  0.11 | err  0.03 | 
16:01:44-scGPT-INFO-train: | epoch   1 | 11600/15494 batches | lr 0.0001 | ms/batch 500.98 | loss  0.13 | cls  0.13 | err  0.04 | 
16:02:34-scGPT-INFO-train: | epoch   1 | 11700/15494 batches | lr 0.0001 | ms/batch 502.06 | loss  0.12 | cls  0.12 | err  0.04 | 
16:03:24-scGPT-INFO-train: | epoch   1 | 11800/15494 batches | lr 0.0001 | ms/batch 499.82 | loss  0.11 | cls  0.11 | err  0.04 | 
16:04:14-scGPT-INFO-train: | epoch   1 | 11900/15494 batches | lr 0.0001 | ms/batch 502.47 | loss  0.11 | cls  0.11 | err  0.03 | 
16:05:05-scGPT-INFO-train: | epoch   1 | 12000/15494 batches | lr 0.0001 | ms/batch 503.39 | loss  0.12 | cls  0.12 | err  0.04 | 
16:05:55-scGPT-INFO-train: | epoch   1 | 12100/15494 batches | lr 0.0001 | ms/batch 500.50 | loss  0.13 | cls  0.13 | err  0.04 | 
16:06:44-scGPT-INFO-train: | epoch   1 | 12200/15494 batches | lr 0.0001 | ms/batch 498.03 | loss  0.12 | cls  0.12 | err  0.04 | 
16:07:35-scGPT-INFO-train: | epoch   1 | 12300/15494 batches | lr 0.0001 | ms/batch 500.42 | loss  0.11 | cls  0.11 | err  0.04 | 
16:08:24-scGPT-INFO-train: | epoch   1 | 12400/15494 batches | lr 0.0001 | ms/batch 497.26 | loss  0.11 | cls  0.11 | err  0.03 | 
16:09:14-scGPT-INFO-train: | epoch   1 | 12500/15494 batches | lr 0.0001 | ms/batch 498.95 | loss  0.11 | cls  0.11 | err  0.04 | 
16:10:05-scGPT-INFO-train: | epoch   1 | 12600/15494 batches | lr 0.0001 | ms/batch 503.72 | loss  0.13 | cls  0.13 | err  0.04 | 
16:10:54-scGPT-INFO-train: | epoch   1 | 12700/15494 batches | lr 0.0001 | ms/batch 498.68 | loss  0.12 | cls  0.12 | err  0.04 | 
16:11:44-scGPT-INFO-train: | epoch   1 | 12800/15494 batches | lr 0.0001 | ms/batch 499.34 | loss  0.12 | cls  0.12 | err  0.04 | 
16:12:35-scGPT-INFO-train: | epoch   1 | 12900/15494 batches | lr 0.0001 | ms/batch 504.28 | loss  0.12 | cls  0.12 | err  0.03 | 
16:13:25-scGPT-INFO-train: | epoch   1 | 13000/15494 batches | lr 0.0001 | ms/batch 500.75 | loss  0.12 | cls  0.12 | err  0.04 | 
16:14:15-scGPT-INFO-train: | epoch   1 | 13100/15494 batches | lr 0.0001 | ms/batch 501.82 | loss  0.12 | cls  0.12 | err  0.03 | 
16:15:05-scGPT-INFO-train: | epoch   1 | 13200/15494 batches | lr 0.0001 | ms/batch 504.71 | loss  0.12 | cls  0.12 | err  0.04 | 
16:15:56-scGPT-INFO-train: | epoch   1 | 13300/15494 batches | lr 0.0001 | ms/batch 501.80 | loss  0.12 | cls  0.12 | err  0.04 | 
16:16:46-scGPT-INFO-train: | epoch   1 | 13400/15494 batches | lr 0.0001 | ms/batch 505.34 | loss  0.12 | cls  0.12 | err  0.04 | 
16:17:36-scGPT-INFO-train: | epoch   1 | 13500/15494 batches | lr 0.0001 | ms/batch 497.41 | loss  0.12 | cls  0.12 | err  0.03 | 
16:18:26-scGPT-INFO-train: | epoch   1 | 13600/15494 batches | lr 0.0001 | ms/batch 498.63 | loss  0.10 | cls  0.10 | err  0.03 | 
16:19:16-scGPT-INFO-train: | epoch   1 | 13700/15494 batches | lr 0.0001 | ms/batch 502.24 | loss  0.11 | cls  0.11 | err  0.04 | 
16:20:06-scGPT-INFO-train: | epoch   1 | 13800/15494 batches | lr 0.0001 | ms/batch 501.91 | loss  0.11 | cls  0.11 | err  0.03 | 
16:20:56-scGPT-INFO-train: | epoch   1 | 13900/15494 batches | lr 0.0001 | ms/batch 500.82 | loss  0.11 | cls  0.11 | err  0.04 | 
16:21:47-scGPT-INFO-train: | epoch   1 | 14000/15494 batches | lr 0.0001 | ms/batch 504.83 | loss  0.11 | cls  0.11 | err  0.03 | 
16:22:37-scGPT-INFO-train: | epoch   1 | 14100/15494 batches | lr 0.0001 | ms/batch 498.28 | loss  0.12 | cls  0.12 | err  0.04 | 
16:23:27-scGPT-INFO-train: | epoch   1 | 14200/15494 batches | lr 0.0001 | ms/batch 503.01 | loss  0.13 | cls  0.13 | err  0.04 | 
16:24:18-scGPT-INFO-train: | epoch   1 | 14300/15494 batches | lr 0.0001 | ms/batch 505.79 | loss  0.11 | cls  0.11 | err  0.03 | 
16:25:07-scGPT-INFO-train: | epoch   1 | 14400/15494 batches | lr 0.0001 | ms/batch 490.59 | loss  0.12 | cls  0.12 | err  0.04 | 
16:25:57-scGPT-INFO-train: | epoch   1 | 14500/15494 batches | lr 0.0001 | ms/batch 500.29 | loss  0.10 | cls  0.10 | err  0.03 | 
16:26:46-scGPT-INFO-train: | epoch   1 | 14600/15494 batches | lr 0.0001 | ms/batch 492.75 | loss  0.13 | cls  0.13 | err  0.04 | 
16:27:36-scGPT-INFO-train: | epoch   1 | 14700/15494 batches | lr 0.0001 | ms/batch 502.07 | loss  0.12 | cls  0.12 | err  0.03 | 
16:28:26-scGPT-INFO-train: | epoch   1 | 14800/15494 batches | lr 0.0001 | ms/batch 497.81 | loss  0.12 | cls  0.12 | err  0.03 | 
16:29:16-scGPT-INFO-train: | epoch   1 | 14900/15494 batches | lr 0.0001 | ms/batch 504.22 | loss  0.13 | cls  0.13 | err  0.04 | 
16:30:07-scGPT-INFO-train: | epoch   1 | 15000/15494 batches | lr 0.0001 | ms/batch 504.72 | loss  0.13 | cls  0.13 | err  0.04 | 
16:30:57-scGPT-INFO-train: | epoch   1 | 15100/15494 batches | lr 0.0001 | ms/batch 499.28 | loss  0.12 | cls  0.12 | err  0.04 | 
16:31:47-scGPT-INFO-train: | epoch   1 | 15200/15494 batches | lr 0.0001 | ms/batch 501.94 | loss  0.12 | cls  0.12 | err  0.03 | 
16:32:37-scGPT-INFO-train: | epoch   1 | 15300/15494 batches | lr 0.0001 | ms/batch 498.47 | loss  0.11 | cls  0.11 | err  0.03 | 
16:33:26-scGPT-INFO-train: | epoch   1 | 15400/15494 batches | lr 0.0001 | ms/batch 496.43 | loss  0.11 | cls  0.11 | err  0.04 | 
16:39:17-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
16:39:17-scGPT-INFO-<module>: | end of epoch   1 | time: 8112.05s | valid loss/mse 0.1360 | err 0.0396
16:39:17-scGPT-INFO-<module>: -----------------------------------------------------------------------------------------
16:39:17-scGPT-INFO-<module>: Best model with score 0.1360
16:51:43-scGPT-INFO-test: Accuracy: 0.960, Precision: 0.805, Recall: 0.804, Macro F1: 0.776
